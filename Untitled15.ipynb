{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfa8f98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoSuchElementException, TimeoutException, WebDriverException\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "driver_path = 'path/to/chromedriver'  # Update this with your actual path\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "try:\n",
    "    # Navigate to BCCI's website\n",
    "    driver.get('https://www.bcci.tv/international/fixtures')\n",
    "    \n",
    "    time.sleep(3)  # Let the page load\n",
    "\n",
    "    # Scraping fixture details\n",
    "    fixtures = driver.find_elements(By.CSS_SELECTOR, '.fixture-card')  # Update CSS selectors based on the website\n",
    "\n",
    "    for fixture in fixtures:\n",
    "        try:\n",
    "            series = fixture.find_element(By.CSS_SELECTOR, '.fixture-card__series').text\n",
    "            place = fixture.find_element(By.CSS_SELECTOR, '.fixture-card__venue').text\n",
    "            date = fixture.find_element(By.CSS_SELECTOR, '.fixture-card__date').text\n",
    "            time = fixture.find_element(By.CSS_SELECTOR, '.fixture-card__time').text\n",
    "            print(f\"Series: {series}, Place: {place}, Date: {date}, Time: {time}\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"Some details are missing for this fixture.\")\n",
    "\n",
    "except (TimeoutException, WebDriverException) as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19321c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     23\u001b[0m     cols \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     rank \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m     gsdp_18_19 \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"http://statisticstimes.com/economy/india/indian-states-gdp.php\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table with the relevant data\n",
    "table = soup.find('table', {'id': 'table_id'})\n",
    "\n",
    "# Lists to store scraped data\n",
    "data = []\n",
    "\n",
    "# Scrape the table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    rank = cols[0].text.strip()\n",
    "    state = cols[1].text.strip()\n",
    "    gsdp_18_19 = cols[2].text.strip()\n",
    "    gsdp_19_20 = cols[3].text.strip()\n",
    "    share_18_19 = cols[4].text.strip()\n",
    "    gdp_billion = cols[5].text.strip()\n",
    "    \n",
    "    # Store the data in a list of dictionaries\n",
    "    data.append({\n",
    "        'Rank': rank,\n",
    "        'State': state,\n",
    "        'GSDP(18-19)': gsdp_18_19,\n",
    "        'GSDP(19-20)': gsdp_19_20,\n",
    "        'Share(18-19)': share_18_19,\n",
    "        'GDP($ billion)': gdp_billion\n",
    "    })\n",
    "\n",
    "# Display scraped data\n",
    "for entry in data:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d0d64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Loop through repositories and extract details\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repos:\n\u001b[1;32m---> 18\u001b[0m     title \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3 lh-condensed\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     19\u001b[0m     description \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol-9 text-gray my-1 pr-4\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo description\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m     language \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, itemprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogrammingLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m repo\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, itemprop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogrammingLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo language specified\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of GitHub trending page\n",
    "url = \"https://github.com/trending\"\n",
    "\n",
    "# Fetch the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all repositories listed on the page\n",
    "repos = soup.find_all('article', class_='Box-row')\n",
    "\n",
    "# Loop through repositories and extract details\n",
    "for repo in repos:\n",
    "    title = repo.find('h1', class_='h3 lh-condensed').text.strip()\n",
    "    description = repo.find('p', class_='col-9 text-gray my-1 pr-4').text.strip() if repo.find('p') else 'No description'\n",
    "    language = repo.find('span', itemprop='programmingLanguage').text.strip() if repo.find('span', itemprop='programmingLanguage') else 'No language specified'\n",
    "    contributors_url = \"https://github.com\" + repo.find_all('a')[1]['href']\n",
    "    \n",
    "    print(f\"Repository Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Language: {language}\")\n",
    "    print(f\"Contributors URL: {contributors_url}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baacf256",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m songs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-chart-results-list__item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m songs:\n\u001b[1;32m---> 15\u001b[0m     song_name \u001b[38;5;241m=\u001b[39m song\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-title\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     artist_name \u001b[38;5;241m=\u001b[39m song\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-label\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     last_week \u001b[38;5;241m=\u001b[39m song\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-label\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m song\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-label\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Billboard Hot 100 chart page\n",
    "url = \"https://www.billboard.com/charts/hot-100\"\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract song data\n",
    "songs = soup.find_all('li', class_='o-chart-results-list__item')\n",
    "\n",
    "for song in songs:\n",
    "    song_name = song.find('h3', class_='c-title').text.strip()\n",
    "    artist_name = song.find('span', class_='c-label').text.strip()\n",
    "    last_week = song.find('span', class_='c-label').text.strip() if song.find('span', class_='c-label') else 'N/A'\n",
    "    peak_rank = song.find('span', class_='c-label').text.strip() if song.find('span', class_='c-label') else 'N/A'\n",
    "    weeks_on_board = song.find('span', class_='c-label').text.strip() if song.find('span', class_='c-label') else 'N/A'\n",
    "\n",
    "    print(f\"Song: {song_name}, Artist: {artist_name}, Last Week Rank: {last_week}, Peak Rank: {peak_rank}, Weeks on Board: {weeks_on_board}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77823177",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m cols \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m book_name \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 21\u001b[0m author_name \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     22\u001b[0m volumes_sold \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m publisher \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Locate the table with book details\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract rows\n",
    "rows = table.find_all('tr')[1:]  # Skip header row\n",
    "\n",
    "# Loop through each row and extract data\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    book_name = cols[0].text.strip()\n",
    "    author_name = cols[1].text.strip()\n",
    "    volumes_sold = cols[2].text.strip()\n",
    "    publisher = cols[3].text.strip()\n",
    "    genre = cols[4].text.strip()\n",
    "\n",
    "    # Print extracted details\n",
    "    print(f\"Book: {book_name}, Author: {author_name}, Volumes Sold: {volumes_sold}, Publisher: {publisher}, Genre: {genre}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18233b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all TV series listings\n",
    "series_list = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "# Loop through each listing and extract details\n",
    "for series in series_list:\n",
    "    name = series.find('h3', class_='lister-item-header').find('a').text.strip()\n",
    "    year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    runtime = series.find('span', class_='runtime').text.strip() if series.find('span', class_='runtime') else 'N/A'\n",
    "    rating = series.find('span', class_='ipl-rating-star__rating').text.strip() if series.find('span', class_='ipl-rating-star__rating') else 'N/A'\n",
    "    votes = series.find('span', attrs={'name': 'nv'}).text.strip() if series.find('span', attrs={'name': 'nv'}) else 'N/A'\n",
    "    \n",
    "    # Print the extracted details\n",
    "    print(f\"Name: {name}, Year Span: {year_span}, Genre: {genre}, Runtime: {runtime}, Rating: {rating}, Votes: {votes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda65195",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Find the dataset table\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m5\u001b[39m]  \u001b[38;5;66;03m# Table containing datasets\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extract dataset details\u001b[39;00m\n\u001b[0;32m     15\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the dataset table\n",
    "table = soup.find_all('table')[5]  # Table containing datasets\n",
    "\n",
    "# Extract dataset details\n",
    "rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    dataset_name = cols[0].text.strip()\n",
    "    data_type = cols[1].text.strip()\n",
    "    task = cols[2].text.strip()\n",
    "    attribute_type = cols[3].text.strip()\n",
    "    no_of_instances = cols[4].text.strip()\n",
    "    no_of_attributes = cols[5].text.strip()\n",
    "    year = cols[6].text.strip()\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}, Data Type: {data_type}, Task: {task}, Attribute Type: {attribute_type}, Instances: {no_of_instances}, Attributes: {no_of_attributes}, Year: {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9ba26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
